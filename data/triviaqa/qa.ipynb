{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75da5cae97fa4c92ae1632e319e3a7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    QuantoConfig\n",
    ")\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_PATH = \"/Model/meta-llama/Meta-Llama-3.1-8B-Instruct-hf\"\n",
    "\n",
    "model_name = os.path.basename(MODEL_PATH)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = 'left'\n",
    "\n",
    "quant_config = QuantoConfig(weights='float8')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map='auto',\n",
    "    attn_implementation=\"eager\",\n",
    "    quantization_config=quant_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliases': ['Park Grove (1895)',\n",
       "  'York UA',\n",
       "  'Yorkish',\n",
       "  'UN/LOCODE:GBYRK',\n",
       "  'York, UK',\n",
       "  'Eoforwic',\n",
       "  'Park Grove School',\n",
       "  'York Ham',\n",
       "  'The weather in York',\n",
       "  'City of York',\n",
       "  'York, England',\n",
       "  'York, Yorkshire',\n",
       "  'York ham',\n",
       "  'County Borough of York',\n",
       "  'YORK',\n",
       "  'Eoferwic',\n",
       "  'Park Grove Primary School',\n",
       "  'York, North Yorkshire',\n",
       "  'Yoisk',\n",
       "  'York',\n",
       "  'York (England)'],\n",
       " 'normalized_aliases': ['york yorkshire',\n",
       "  'eoferwic',\n",
       "  'park grove primary school',\n",
       "  'park grove school',\n",
       "  'weather in york',\n",
       "  'park grove 1895',\n",
       "  'eoforwic',\n",
       "  'county borough of york',\n",
       "  'york uk',\n",
       "  'un locode gbyrk',\n",
       "  'city of york',\n",
       "  'york england',\n",
       "  'york ua',\n",
       "  'york ham',\n",
       "  'york',\n",
       "  'yorkish',\n",
       "  'yoisk',\n",
       "  'york north yorkshire'],\n",
       " 'matched_wiki_entity_name': '',\n",
       " 'normalized_matched_wiki_entity_name': '',\n",
       " 'normalized_value': 'york',\n",
       " 'type': 'WikipediaEntity',\n",
       " 'value': 'York'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/Dataset/mandarjoshi/trivia_qa\")\n",
    "dataset['train'][1]['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, template, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset[ind]\n",
    "        text = template.format(question=rec['question']) + \" \" + rec['answer']['normalized_value']\n",
    "        # print(rec['answer'])\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "few_shot_rng = np.random.RandomState(42)\n",
    "n_shots =  5\n",
    "few_shot_prompt = get_few_shot_prompt(dataset['train'], TEMPLATE, few_shot_rng, n_shots)\n",
    "# print(few_shot_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "from tqdm import tqdm\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def handle_punc(text):\n",
    "        exclude = set(string.punctuation + \"\".join([u\"‘\", u\"’\", u\"´\", u\"`\"]))\n",
    "        return ''.join(ch if ch not in exclude else ' ' for ch in text)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def replace_underscore(text):\n",
    "        return text.replace('_', ' ')\n",
    "\n",
    "    return white_space_fix(remove_articles(handle_punc(lower(replace_underscore(s))))).strip()\n",
    "\n",
    "\n",
    "def has_exact_match(ground_truths, candidates):\n",
    "    for ground_truth in ground_truths:\n",
    "        if ground_truth in candidates:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "n = 50\n",
    "acc = 0\n",
    "\n",
    "stop_words = [\"\\n\", \".\", \",\"]\n",
    "genconfig = GenerationConfig(max_new_tokens=24, do_sample=False, stop_strings=stop_words, pad_token_id=tok.eos_token_id)\n",
    "pipe = TextGenerationPipeline(model=model, tokenizer=tok)\n",
    "\n",
    "prompts = []\n",
    "for i in range(n):\n",
    "    rec = dataset['validation'][i]\n",
    "    few_shot_prompt = get_few_shot_prompt(dataset['train'], TEMPLATE, few_shot_rng, n_shots)\n",
    "    prompt = few_shot_prompt + TEMPLATE.format(question=rec['question'])\n",
    "    # prompt = tok.apply_chat_template([{'role': 'user', 'content': prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "generated_answers = pipe(prompts, tokenizer=tok, return_full_text=False, clean_up_tokenization_spaces=True, prefix=tok.bos_token, generation_config=genconfig)\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    rec = dataset['validation'][i]\n",
    "    answer = answer[0]['generated_text'].strip(\"\".join(stop_words))\n",
    "    normalized_answer = normalize_answer(answer)\n",
    "    # print(normalized_answer, \"|\", \",\".join(rec['answer']['normalized_aliases']))\n",
    "    if normalized_answer in rec['answer']['normalized_aliases']:\n",
    "        acc += 1\n",
    "acc/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Where in England was Dame Judi Dench born?',\n",
       " 'question_id': 'tc_3',\n",
       " 'question_source': 'http://www.triviacountry.com/',\n",
       " 'entity_pages': {'doc_source': [],\n",
       "  'filename': [],\n",
       "  'title': [],\n",
       "  'wiki_context': []},\n",
       " 'search_results': {'description': [],\n",
       "  'filename': [],\n",
       "  'rank': [],\n",
       "  'title': [],\n",
       "  'url': [],\n",
       "  'search_context': []},\n",
       " 'answer': {'aliases': ['Park Grove (1895)',\n",
       "   'York UA',\n",
       "   'Yorkish',\n",
       "   'UN/LOCODE:GBYRK',\n",
       "   'York, UK',\n",
       "   'Eoforwic',\n",
       "   'Park Grove School',\n",
       "   'York Ham',\n",
       "   'The weather in York',\n",
       "   'City of York',\n",
       "   'York, England',\n",
       "   'York, Yorkshire',\n",
       "   'York ham',\n",
       "   'County Borough of York',\n",
       "   'YORK',\n",
       "   'Eoferwic',\n",
       "   'Park Grove Primary School',\n",
       "   'York, North Yorkshire',\n",
       "   'Yoisk',\n",
       "   'York',\n",
       "   'York (England)'],\n",
       "  'normalized_aliases': ['york yorkshire',\n",
       "   'eoferwic',\n",
       "   'park grove primary school',\n",
       "   'park grove school',\n",
       "   'weather in york',\n",
       "   'park grove 1895',\n",
       "   'eoforwic',\n",
       "   'county borough of york',\n",
       "   'york uk',\n",
       "   'un locode gbyrk',\n",
       "   'city of york',\n",
       "   'york england',\n",
       "   'york ua',\n",
       "   'york ham',\n",
       "   'york',\n",
       "   'yorkish',\n",
       "   'yoisk',\n",
       "   'york north yorkshire'],\n",
       "  'matched_wiki_entity_name': '',\n",
       "  'normalized_matched_wiki_entity_name': '',\n",
       "  'normalized_value': 'york',\n",
       "  'type': 'WikipediaEntity',\n",
       "  'value': 'York'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/Dataset/mandarjoshi/trivia_qa\")\n",
    "dataset['train'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results = []\n",
    "# filename = \"results/Meta-Llama-3.1-8B-hf-seed=0-temperature=1.0-top_k=50-num_seq=50-shots=5.jsonl\"\n",
    "# filename = \"results/Llama-2-13b-hf-seed=0-temperature=1.0-top_k=50-num_seq=50-shots=5.jsonl\"\n",
    "filename = \"results/Meta-Llama-3.1-8B-hf-seed=0-temperature=1.0-top_k=50-num_seq=20-shots=20.jsonl\"\n",
    "with open(filename, 'r') as fp:\n",
    "    for line in fp:\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "            results.append(rec)\n",
    "        except:break\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = filename.split(\"-\")\n",
    "for x in filename:\n",
    "    if \"num_seq\" in x:\n",
    "        num_answers = int(x.split('=')[-1])\n",
    "num_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5529"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def handle_punc(text):\n",
    "        exclude = set(string.punctuation + \"\".join([u\"‘\", u\"’\", u\"´\", u\"`\"]))\n",
    "        return ''.join(ch if ch not in exclude else ' ' for ch in text)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def replace_underscore(text):\n",
    "        return text.replace('_', ' ')\n",
    "\n",
    "    return white_space_fix(remove_articles(handle_punc(lower(replace_underscore(s))))).strip()\n",
    "\n",
    "acc = 0\n",
    "n = num_answers * len(results)\n",
    "for i in range(len(results)):\n",
    "    answers = results[i]\n",
    "    rec = dataset['validation'][i]\n",
    "    for ans in answers:\n",
    "        na = normalize_answer(ans)\n",
    "        # if na in rec['answer']['normalized_aliases']:\n",
    "        #     acc += 1\n",
    "        if any(a in na for a in rec['answer']['normalized_aliases']):\n",
    "            acc += 1\n",
    "acc/n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reptda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
