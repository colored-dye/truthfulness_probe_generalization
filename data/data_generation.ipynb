{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv(\"inventors_disj_new.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "new_csv = copy.deepcopy(csv)\n",
    "\n",
    "for i, rec in csv.iterrows():\n",
    "    stmt = rec['statement']\n",
    "    city_start = len(\"It is the case either that \")\n",
    "    city_end = stmt.index(\"lived\")\n",
    "    # print(stmt)\n",
    "    # print(stmt[city_start:city_end])\n",
    "\n",
    "    or_that = stmt.index(\"or that \") + 8\n",
    "    after_it = stmt[or_that:]\n",
    "    # stmt = stmt[:or_that] + stmt[city_start:city_end] + after_it\n",
    "    stmt = stmt[:city_start] + \"the inventor \" + stmt[city_start:or_that] + \"the inventor \" + after_it\n",
    "    \n",
    "    new_csv.loc[i, ['statement']] = copy.copy(stmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is the case either that the inventor Tim Berners-Lee lived in Germany or that the inventor Tim Berners-Lee lived in the U.S.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_csv['statement'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv.to_csv(\"inventors_disj_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "neg_companies = pd.read_csv(\"neg_companies.csv\")\n",
    "neg_companies = pd.DataFrame({\n",
    "    'statement': neg_companies['statement'].tolist(),\n",
    "    'label': [1-l for l in neg_companies['label'].tolist()],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_companies.to_csv(\"neg_companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gpt_results = pd.read_csv(\"factuality_judgment.csv\")\n",
    "companies = pd.read_csv(\"companies.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gpt_results = np.array([int(i) for i in gpt_results['label'].tolist()])\n",
    "companies = np.array([int(i) for i in companies['label'].tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020000000000000018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_o = np.count_nonzero(gpt_results == companies) / len(gpt_results)\n",
    "p_yes = np.count_nonzero(gpt_results==1)/len(gpt_results) * np.count_nonzero(companies==1)/len(companies)\n",
    "p_no = np.count_nonzero(gpt_results==0)/len(gpt_results) * np.count_nonzero(companies==0)/len(companies)\n",
    "p_e = p_yes + p_no\n",
    "kappa = (p_o-p_e) / (1-p_e)\n",
    "kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(companies==gpt_results)/len(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf4f2f7852d478fa011618622a7a704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    QuantoConfig\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_PATH = \"/Model/meta-llama/Meta-Llama-3.1-8B-Instruct-hf\"\n",
    "\n",
    "model_name = os.path.basename(MODEL_PATH)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = 'left'\n",
    "\n",
    "quant_config = QuantoConfig(weights='float8')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    attn_implementation=\"eager\",\n",
    "    quantization_config=quant_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"element_symb_disj.csv\")\n",
    "\n",
    "answers = []\n",
    "for i, rec in tqdm(csv.iterrows(), total=len(csv)):\n",
    "    stmt = rec['statement']\n",
    "    msg = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant. When asked to judge the factuality of a statement, you need to consider the statement as a whole.\n",
    "For example, when asked \\\"Is the following statement true or false?\\nIt is the case either that Molybdenum has the symbol Mo or that Molybdenum has the symbol In.\\\", you should answer \\\"True\\\", because Molybdenum has the symbol Mo, even if it does not has the symbol In.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"Is the following statement true or false?\\n\" + stmt},\n",
    "    ]\n",
    "    ids = tok.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "    ids = tok(ids, return_tensors='pt').to(DEVICE)\n",
    "    outputs = model.generate(**ids, do_sample=False, max_new_tokens=32, pad_token_id=tok.eos_token_id, top_p=None, temperature=None, top_k=None)\n",
    "    ans = tok.decode(outputs[0, ids.input_ids.shape[1]:], skip_special_tokens=True).lower().strip(\" \\n\")\n",
    "    answers.append(0 if \"false\" in ans else 1)\n",
    "    # print(ans, rec['label'])\n",
    "    # if i == 2:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Is the following statement true or false? Answer a single word \"true\" or \"false\".\n",
      "\n",
      "either 'Niobium has the symbol Nb' or 'Niobium has the symbol Cl' is true.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "False<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = pd.DataFrame({\n",
    "    \"statement\": csv['statement'].tolist(),\n",
    "    \"label\": answers\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile.to_csv(\"element_symb_disj_llama3_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa(x, y):\n",
    "    p_o = np.count_nonzero(x == y) / len(x)\n",
    "    p_yes = np.count_nonzero(x==1)/len(x) * np.count_nonzero(y==1)/len(y)\n",
    "    p_no = np.count_nonzero(x==0)/len(x) * np.count_nonzero(y==0)/len(y)\n",
    "    p_e = p_yes + p_no\n",
    "    return (p_o-p_e) / (1-p_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9767441860465116, 0.9518477043673012)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_answers = np.array(csv['label'].tolist())\n",
    "llama_answers = np.array(answers)\n",
    "\n",
    "np.count_nonzero(original_answers==llama_answers)/original_answers.size, kappa(original_answers, llama_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?',\n",
       " 'distractor3': 'residues',\n",
       " 'distractor1': 'antioxidants',\n",
       " 'distractor2': 'Oxygen',\n",
       " 'correct_answer': 'oxidants',\n",
       " 'support': 'Oxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F2, are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = \"/Dataset/allenai/sciq\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "dataset['test'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who proposed the theory of evolution by natural selection?',\n",
       " 'distractor3': 'Scopes',\n",
       " 'distractor1': 'Linnaeus',\n",
       " 'distractor2': 'shaw',\n",
       " 'correct_answer': 'darwin',\n",
       " 'support': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Context: {context}\n",
    "Question: {question}\n",
    "Options:\n",
    "- {c1}\n",
    "- {c2}\n",
    "- {c3}\n",
    "- {c4}\n",
    "Answer: {answer}\"\"\"\n",
    "sciq_true_false = []\n",
    "rng = np.random.RandomState(0)\n",
    "few_shot_rng = np.random.RandomState(1)\n",
    "n_shots = 3\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset[ind]\n",
    "        choices = ['correct_answer', 'distractor1', 'distractor2', 'distractor3']\n",
    "        rng.shuffle(choices)\n",
    "        if i != -1:\n",
    "            text = template.format(question=rec['question'], context=rec['support'], answer=rec['correct_answer'], c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]])\n",
    "        else:\n",
    "            text = template.format(question=rec['question'], context=rec['support'], answer=rec['distractor1'], c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]])\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "for rec in dataset['test']:\n",
    "    choices = ['correct_answer', 'distractor1', 'distractor2', 'distractor3']\n",
    "    rng.shuffle(choices)\n",
    "    \n",
    "    sciq_true_false.append({\n",
    "        \"statement\": get_few_shot_prompt(dataset['validation'], few_shot_rng, n_shots) + template.format(question=rec['question'], context=rec['support'], answer=rec['correct_answer'], c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]]),\n",
    "        \"label\": 1,\n",
    "    })\n",
    "    for i in range(1, 4):\n",
    "        rng.shuffle(choices)\n",
    "        sciq_true_false.append({\n",
    "            \"statement\": get_few_shot_prompt(dataset['validation'], few_shot_rng, n_shots) + template.format(question=rec['question'], context=rec['support'], answer=rec[f'distractor{i}'], c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]]),\n",
    "            \"label\": 0,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Context: {context}\n",
    "Question: {question}\n",
    "Options:\n",
    "A. {c1}\n",
    "B. {c2}\n",
    "C. {c3}\n",
    "D. {c4}\n",
    "Answer: {answer}\"\"\"\n",
    "sciq_true_false = []\n",
    "rng = np.random.RandomState(0)\n",
    "few_shot_rng = np.random.RandomState(1)\n",
    "n_shots = 3\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset[ind]\n",
    "        choices = ['correct_answer', 'distractor1', 'distractor2', 'distractor3']\n",
    "        rng.shuffle(choices)\n",
    "        if i >= 2:\n",
    "            text = template.format(question=rec['question'], context=rec['support'], answer=chr(65+choices.index('correct_answer')), c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]])\n",
    "        else:\n",
    "            text = template.format(question=rec['question'], context=rec['support'], answer=chr(65 + choices.index('distractor1')), c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]])\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "for rec in dataset['test']:\n",
    "    choices = ['correct_answer', 'distractor1', 'distractor2', 'distractor3']\n",
    "    rng.shuffle(choices)\n",
    "    \n",
    "    fs_prompt = get_few_shot_prompt(dataset['validation'], few_shot_rng, n_shots)\n",
    "    # fs_prompt = \"\"\n",
    "    sciq_true_false.append({\n",
    "        \"statement\": fs_prompt + template.format(question=rec['question'], context=rec['support'], answer=chr(65+choices.index('correct_answer')), c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]]),\n",
    "        \"label\": 1,\n",
    "    })\n",
    "    for i in range(1, 4):\n",
    "        rng.shuffle(choices)\n",
    "        fs_prompt = get_few_shot_prompt(dataset['validation'], few_shot_rng, n_shots)\n",
    "        # fs_prompt = \"\"\n",
    "        sciq_true_false.append({\n",
    "            \"statement\": fs_prompt + template.format(question=rec['question'], context=rec['support'], answer=chr(65 + choices.index(f'distractor{i}')), c1=rec[choices[0]], c2=rec[choices[1]], c3=rec[choices[2]], c4=rec[choices[3]]),\n",
    "            \"label\": 0,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Recall that boiling occurs when the vapor pressure of a liquid is equal to the atmospheric pressure. Since adding a solute lowers the vapor pressure, we would expect a higher temperature to be required before boiling can begin. This phenomenon, known as boiling point elevation , occurs whenever a solute is dissolved into a pure solvent.\n",
      "Question: What occurs when the vapor pressure of a liquid is equal to the atmospheric pressure?\n",
      "Options:\n",
      "A. freezing\n",
      "B. boiling\n",
      "C. melting\n",
      "D. evaporation\n",
      "Answer: A\n",
      "\n",
      "Context: The pea plant Pisum sativum has purple and white flowers. These flowered plants are not just pretty to look at. These plants led Gregor Mendel to unlock the secrets of heredity, beginning the field of genetics. For his efforts, Mendel is widely known as the Father of Genetics, even though he knew nothing of the genetic material, DNA. The laws he developed apply to all sexually reproducing life, and are the basis for beginning to understand many human diseases.\n",
      "Question: Who is widely known as the father of genetics?\n",
      "Options:\n",
      "A. james watson\n",
      "B. gregor mendel\n",
      "C. francis crick\n",
      "D. walter gehring\n",
      "Answer: D\n",
      "\n",
      "Context: Organs of the lymphatic system include the tonsils, thymus gland and spleen. The thymus gland produces T cells or T-lymphocytes (see below) and the spleen and tonsils help in fighting infections. The spleen’s main function is to filter the blood, removing unwanted red blood cells. The spleen also detects viruses and bacteria and triggers the release of pathogen fighting cells.\n",
      "Question: The main function of this organ is to filter the blood and remove unwanted red blood cells?\n",
      "Options:\n",
      "A. kidney\n",
      "B. pancreas\n",
      "C. heart\n",
      "D. spleen\n",
      "Answer: D\n",
      "\n",
      "Context: Oxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F2, are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books.\n",
      "Question: Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?\n",
      "Options:\n",
      "A. Oxygen\n",
      "B. residues\n",
      "C. antioxidants\n",
      "D. oxidants\n",
      "Answer: D\n"
     ]
    }
   ],
   "source": [
    "print(sciq_true_false[0]['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sciq_csv = pd.DataFrame({\n",
    "    \"statement\": [rec['statement'] for rec in sciq_true_false],\n",
    "    \"label\": [rec['label'] for rec in sciq_true_false],\n",
    "})\n",
    "sciq_csv.to_csv(\"sciq_true_false_mc_FFT.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "mmlu_base_path = \"/Dataset/mmlu\"\n",
    "\n",
    "def load_mmlu_split(mmlu_base_path, split):\n",
    "    mmlu_split_path = os.path.join(mmlu_base_path, split)\n",
    "    ds = {}\n",
    "    for f in os.listdir(mmlu_split_path):\n",
    "        task = f.replace(\".csv\", \"\").replace(f\"_{split}\", \"\")\n",
    "        csv = pd.read_csv(os.path.join(mmlu_split_path, f), names=(\"question\", \"A\", \"B\", \"C\", \"D\", \"answer\"))\n",
    "        ds[task] = csv\n",
    "    return ds\n",
    "\n",
    "dev_set = load_mmlu_split(mmlu_base_path, \"dev\")\n",
    "test_set = load_mmlu_split(mmlu_base_path, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_NO_OPTIONS = \"\"\"Question: {question}\n",
    "Answer: {answer}\"\"\"\n",
    "\n",
    "TEMPLATE_WITH_OPTIONS = \"\"\"Question: {question}\n",
    "Options:\n",
    "- {c1}\n",
    "- {c2}\n",
    "- {c3}\n",
    "- {c4}\n",
    "Answer: {answer}\"\"\"\n",
    "\n",
    "mmlu_true_false = []\n",
    "n_shots = 5\n",
    "select_rng = np.random.RandomState(42)\n",
    "few_shot_rng = np.random.RandomState(0)\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset.iloc[ind]\n",
    "        if i <= 1:\n",
    "            text = TEMPLATE_WITH_OPTIONS.format(question=rec['question'], answer=rec[rec['answer']], c1=rec['A'], c2=rec['B'], c3=rec['C'], c4=rec['D'])\n",
    "        else:\n",
    "            text = TEMPLATE_WITH_OPTIONS.format(question=rec['question'], answer=rec[chr(ord('A') + ((ord(rec['answer'])-ord('A')+1)%4))], c1=rec['A'], c2=rec['B'], c3=rec['C'], c4=rec['D'])\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "for task, csv in test_set.items():\n",
    "    csv = csv.iloc[select_rng.choice(len(csv), 50, replace=False)]\n",
    "    for _, rec in csv.iterrows():\n",
    "        for i in range(4):\n",
    "            choice = chr(ord('A')+i)\n",
    "            text = TEMPLATE_WITH_OPTIONS.format(question=rec['question'], c1=rec['A'], c2=rec['B'], c3=rec['C'], c4=rec['D'], answer=rec[choice])\n",
    "            few_shot_prompt = get_few_shot_prompt(dev_set[task], few_shot_rng, n_shots)\n",
    "            text = few_shot_prompt + text\n",
    "            mmlu_true_false.append({\n",
    "                \"statement\": text,\n",
    "                \"label\": 1 if choice == rec['answer'] else 0,\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_MC = \"\"\"Question: {question}\n",
    "A. {c1}\n",
    "B. {c2}\n",
    "C. {c3}\n",
    "D. {c4}\n",
    "Answer: {answer}\"\"\"\n",
    "\n",
    "mmlu_true_false = []\n",
    "n_shots = 5\n",
    "select_rng = np.random.RandomState(42)\n",
    "few_shot_rng = np.random.RandomState(0)\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset.iloc[ind]\n",
    "        if i <= 5:\n",
    "            text = TEMPLATE_MC.format(question=rec['question'], answer=rec['answer'], c1=rec['A'], c2=rec['B'], c3=rec['C'], c4=rec['D'])\n",
    "        else:\n",
    "            text = TEMPLATE_MC.format(question=rec['question'], answer=chr(ord('A') + ((ord(rec['answer'])-ord('A')+1)%4)), c1=rec['A'], c2=rec['B'], c3=rec['C'], c4=rec['D'])\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "for task, csv in test_set.items():\n",
    "    csv = csv.iloc[select_rng.choice(len(csv), 50, replace=False)]\n",
    "    for _, rec in csv.iterrows():\n",
    "        for i in range(4):\n",
    "            choice = chr(ord('A')+i)\n",
    "            text = TEMPLATE_MC.format(question=rec['question'], c1=rec['A'], c2=rec['B'], c3=rec['C'], c4=rec['D'], answer=choice)\n",
    "            few_shot_prompt = get_few_shot_prompt(dev_set[task], few_shot_rng, n_shots)\n",
    "            text = few_shot_prompt + text\n",
    "            mmlu_true_false.append({\n",
    "                \"statement\": text,\n",
    "                \"label\": 1 if choice == rec['answer'] else 0,\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: From the solubility rules, which of the following is true?\n",
      "A. All chlorides, bromides, and iodides are soluble\n",
      "B. All sulfates are soluble\n",
      "C. All hydroxides are soluble\n",
      "D. All ammonium-containing compounds are soluble\n",
      "Answer: D\n",
      "\n",
      "Question: Which of the following is considered an acid anhydride?\n",
      "A. HCl\n",
      "B. H2SO3\n",
      "C. SO2\n",
      "D. Al(NO3)3\n",
      "Answer: C\n",
      "\n",
      "Question: Which of the following is expected to be a polar molecule?\n",
      "A. PCl4F\n",
      "B. BF3\n",
      "C. CO2\n",
      "D. Si(CH3)4\n",
      "Answer: A\n",
      "\n",
      "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
      "A. 3.89\n",
      "B. 7.78\n",
      "C. 5.78\n",
      "D. 2.33\n",
      "Answer: C\n",
      "\n",
      "Question: A solution contains 2.00 mole of acetic acid, CH3COOH, and 1.00 mole of calcium acetate, Ca(CH3COO)2. The solution is able to resist the addition of a small amount of strong acid or strong base with only minor changes in the pH of the solution. Larger quantities of strong acid or strong base can cause a significant change in pH. How many moles of nitric acid, HNO3, may be added before the pH begins to change significantly?\n",
      "A. 0.500 mole\n",
      "B. 1.00 mole\n",
      "C. 2.00 mole\n",
      "D. 3.00 mole\n",
      "Answer: C\n",
      "\n",
      "Question: A student has a liter of a 0.100 M solution of a strong acid. To prepare a buffer, this should be mixed with\n",
      "A. a strong acid\n",
      "B. a weak acid\n",
      "C. a weak base\n",
      "D. a strong base\n",
      "Answer: A\n"
     ]
    }
   ],
   "source": [
    "print(mmlu_true_false[0]['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mmlu_true_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.DataFrame({\n",
    "    \"statement\": [rec['statement'] for rec in mmlu_true_false],\n",
    "    \"label\": [rec['label'] for rec in mmlu_true_false],\n",
    "})\n",
    "csv.to_csv(\"mmlu_true_false_mc_TTTTT.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "model_name = \"Meta-Llama-3.1-8B-hf\"\n",
    "# model_name = \"Llama-2-13b-hf\"\n",
    "results_file = f\"triviaqa/results/{model_name}-seed=0-temperature=1.0-top_k=50-num_seq=20-shots=20.jsonl\"\n",
    "\n",
    "answers = []\n",
    "with open(results_file) as fp:\n",
    "    for line in fp:\n",
    "        answers.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'],\n",
       "        num_rows: 138384\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'],\n",
       "        num_rows: 17944\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'],\n",
       "        num_rows: 17210\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"/Dataset/mandarjoshi/trivia_qa\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re, string\n",
    "\n",
    "FEW_SHOT_TEMPLATE = \"\"\"Question: {question}\n",
    "Answer: {answer}\"\"\"\n",
    "\n",
    "GENERATION_TEMPLATE = \"\"\"Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "triviaqa_true_false = []\n",
    "n_shots = 5\n",
    "few_shot_rng = np.random.RandomState(0)\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, template, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset[ind]\n",
    "        text = template.format(question=rec['question'], answer=rec['answer']['normalized_value'])\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def handle_punc(text):\n",
    "        exclude = set(string.punctuation + \"\".join([u\"‘\", u\"’\", u\"´\", u\"`\"]))\n",
    "        return ''.join(ch if ch not in exclude else ' ' for ch in text)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def replace_underscore(text):\n",
    "        return text.replace('_', ' ')\n",
    "\n",
    "    return white_space_fix(remove_articles(handle_punc(lower(replace_underscore(s))))).strip()\n",
    "\n",
    "\n",
    "def has_exact_match(ground_truths, candidates):\n",
    "    for ground_truth in ground_truths:\n",
    "        if ground_truth in candidates:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "for i in range(len(answers)):\n",
    "    few_shot_prompt = get_few_shot_prompt(dataset['train'], FEW_SHOT_TEMPLATE, few_shot_rng, n_shots)\n",
    "    rec = dataset['validation'][i]\n",
    "    for ans in answers[i]:\n",
    "        prompt = few_shot_prompt + FEW_SHOT_TEMPLATE.format(question=rec['question'], answer=ans)\n",
    "        normalized_ans = normalize_answer(ans)\n",
    "        label = 1 if any(a in normalized_ans for a in rec['answer']['normalized_aliases']) else 0\n",
    "        triviaqa_true_false.append({\n",
    "            \"statement\": prompt,\n",
    "            \"label\": label,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.DataFrame({\n",
    "    \"statement\": [rec['statement'] for rec in triviaqa_true_false],\n",
    "    \"label\": [rec['label'] for rec in triviaqa_true_false],\n",
    "})\n",
    "csv.to_csv(f\"triviaqa_true_false_{model_name}-shots=20.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "xsum = load_dataset(\"/Dataset/xsum\")\n",
    "xsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\\nWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\\nThe Welsh Government said more people than ever were getting help to address housing problems.\\nChanges to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.\\nPrison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.\\nHowever, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.\\nAndrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".\\n\"There\\'s a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.\\n\"It could take six months to a year, without a lot of help they could be on the streets for six months.\\n\"When you think of the consequences of either being on the street, especially with the cold weather at the moment or you may have a roof over your head, sometimes there is only one choice.\"\\nMr Stevens believes building more one-bedroom flats could help ease the problem.\\n\"The average price is a hundred pounds a week to keep someone in a rented flat, prison is a lot more than that so I would imagine it would save the public purse quite a few pounds,\" he said.\\nOfficial figures show 830 one-bedroom properties were built in the year to March 2016, of an overall total of 6,900 new properties in Wales.\\nMarc, 50, who has been in and out of prison for the past 20 years for burglary offences, said he struggled to find accommodation each time he was released.\\nHe said he would ask himself: \"Where am I going to stay? Where am I going to live? Have I got somewhere where I can see my daughter.\"\\n\"You\\'re put out among the same sort of people doing the same sort of thing, and it\\'s difficult, it\\'s difficult to get away from it. It\\'s like every man for himself, there\\'s nothing.\"\\nMarc has now found stable accommodation with homeless charity Emmaus and said it had been life changing.\\n\"You feel safe, you got hot food, you\\'ve got company of people in similar situations to yourself but all dealing with different issues. It\\'s a constructive, helpful atmosphere,\" he said.\\nTom Clarke, chief executive of Emmaus South Wales, agreed there was not enough support available.\\n\"We do still see [people] homeless on the streets, so clearly they haven\\'t got accommodation and haven\\'t got provision,\" he said.\\n\"I think the key is connecting people with the services they need. I don\\'t delude myself that Emmaus can offer a one size fits all for everyone, we can\\'t.\\n\"But there must be other opportunities and given suitable encouragement I believe that can and should happen.\"\\nA Welsh Government spokesman said the national pathway for homeless services to children, young people and adults in the secure estate had prevented many people from losing their home whilst serving their prison sentence.\\nIt added there were already significant demands for one-bedroom flats across the public and private sector and it was providing 20,000 new affordable homes in the next five years.',\n",
       " 'summary': 'There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.',\n",
       " 'id': '38264402'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"/Model/meta-llama/Meta-Llama-3.1-8B-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "MAX_LENGTH = 8192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hallu_csv = pd.read_csv(\"/Dataset/xsum_hallucination/hallucination_annotations_xsum_summaries.csv\")\n",
    "ids = set(hallu_csv['bbcid'])\n",
    "hallu_selected = []\n",
    "for _, rec in hallu_csv.iterrows():\n",
    "    if rec['bbcid'] in ids:\n",
    "        for orig in xsum['test']:\n",
    "            if orig['id'] == str(rec['bbcid']):\n",
    "                break\n",
    "        hallu_selected.append({\n",
    "            'id': rec['bbcid'],\n",
    "            'document': orig['document'],\n",
    "            'correct_summary': orig['summary'],\n",
    "            'wrong_summary': rec['summary'],\n",
    "        })\n",
    "        ids.remove(rec['bbcid'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 34687720,\n",
       " 'document': 'France\\'s Dubuisson carded a 67 to tie with overnight leader Van Zyl of South Africa on 16 under par.\\nMcIlroy carded a third straight five under-par 67 to move to 15 under par with Thailand\\'s Kiradech Aphibarnrat.\\nThe world number three\\'s round included an eagle on the 12th as he bids to win his first title since May.\\n\"The 67s I\\'ve shot this week have all been a little different and I feel like I\\'ve played within myself for all of them, \" said four-time major winner McIlroy of Northern Ireland. \"I feel there\\'s a low round out there for me and hopefully it\\'s tomorrow.\"\\nMcIlroy was level par for the day after 10 holes, dropping his first shots of the week by three-putting the third and 10th, the latter mistake prompting the 26-year-old to throw his putter at his bag.\\nBut he hit back with a birdie on the par-five 11th and a towering four iron from 229 yards on the 13th set up an eagle from just four feet.\\nThe former world number one ruptured a ligament in his left ankle during a game of football with friends in July, ruling him out of several tournaments.\\nBut he returned in time to unsuccessfully defend his US PGA title at Whistling Straits in August and played in three of the FedEx Cup play-off events before starting the new PGA Tour season with a tie for 26th in the Frys.com Open in California.\\nHe is targeting a third Race to Dubai title in four years and leads England\\'s Danny Willett by 271, 214 points with three events remaining after the Turkish Open.\\nEnglish pair Chris Wood (-13) and Richard Bland (-12) who were tied for second overnight are fifth and seventh respectively.',\n",
       " 'correct_summary': 'Rory McIlroy moved to within a shot of joint leaders Victor Dubuisson and Jaco van Zyl after the third round of the Turkish Airlines Open.',\n",
       " 'wrong_summary': 'rory mcilroy will take a one-shot lead into the final round of the wgc-hsbc champions after carding a three-under'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallu_selected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14724 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "XSUM_TEMPLATE = \"\"\"Summarize this document: {doc}\n",
    "Summary: {sum}\"\"\"\n",
    "\n",
    "xsum_true_false = []\n",
    "n_shots = 2\n",
    "few_shot_rng = np.random.RandomState(0)\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset[ind]\n",
    "        text = XSUM_TEMPLATE.format(doc=rec['document'].replace(\"\\n\", \" \"), sum=rec['summary'])\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "for rec in hallu_selected:\n",
    "    # pos_stmt = XSUM_TEMPLATE.format(doc=rec['document'].replace(\"\\n\", \" \"), sum=rec['correct_summary'])\n",
    "    # neg_stmt = XSUM_TEMPLATE.format(doc=rec['document'].replace(\"\\n\", \" \"), sum=rec['wrong_summary'])\n",
    "    \n",
    "    few_shot_prompt = get_few_shot_prompt(xsum['train'], few_shot_rng, n_shots)\n",
    "    pos_stmt = few_shot_prompt + XSUM_TEMPLATE.format(doc=rec['document'].replace(\"\\n\", \" \"), sum=rec['correct_summary'])\n",
    "    neg_stmt = few_shot_prompt + XSUM_TEMPLATE.format(doc=rec['document'].replace(\"\\n\", \" \"), sum=rec['wrong_summary'])\n",
    "    if len(tokenizer.encode(pos_stmt)) > MAX_LENGTH or len(tokenizer.encode(neg_stmt)) > MAX_LENGTH:\n",
    "        continue\n",
    "    \n",
    "    xsum_true_false.append({\n",
    "        \"statement\": pos_stmt,\n",
    "        \"label\": 1,\n",
    "    })\n",
    "    xsum_true_false.append({\n",
    "        \"statement\": neg_stmt,\n",
    "        \"label\": 0,\n",
    "    })\n",
    "\n",
    "print(len(xsum_true_false))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize this document: It said that dissidents targeted an army vehicle with an improvised explosive device. Four other soldiers were wounded. The attack has been condemned by President Juan Manuel Santos, who last year won the Nobel Peace Prize for his work on the peace agreement. The deal ended half a century of conflict but renegade Farc fighters have since clashed with other Farc members as well as government forces. Colombia landslide: Farc rebels offer to help rebuild town The Farc rebel rapping for peace Dozens of rights activists killed in Colombia in 2016 The government and the Farc signed a peace agreement in November. Most Farc members are now inside or heading to one of 26 transition camps dotted around the country. They were set up as part of the historic peace deal which ended more than five decades of armed uprising. But some Farc leaders have complained that, despite months of planning, most of the transition camps are still lacking the basic amenities they say they were promised. If the peace process goes to plan, the Farc will transform itself into a political force and its former fighters will re-enter society. But, for many who have little or no formal education, that may be a huge challenge.\n",
      "Summary: Members of the left-wing Colombian Farc rebel group opposed to last year's historic peace agreement have killed a soldier in an attack in the south-east of the country, the army has said.\n",
      "\n",
      "Summarize this document: Jeon Wook-pyo, now 68, was among 25 crewmen aboard two fishing boats captured by North Korea in the Yellow Sea in 1972. He escaped North Korea in August and returned to South Korea this month, Yonhap news agency reported. The North and South remain technically at war after the 1950-53 conflict ended in an armistice and not a peace treaty. A spokesman from South Korea's Unification Ministry, which oversees affairs between the two Koreas, confirmed the man's return but did not offer additional details. Mr Jeon made his way to South Korea via an undisclosed third country, from which he issued an appeal for help to South Korean President Park Geun-hye, Yonhap news agency said, citing an unidentified government official. No information was given about the other crewmen on the boats. Mr Jeon was currently under investigation in the South and would be allowed to return to his family after that was complete, the official was quoted by Yonhap as saying. There is a great deal of suspicion surrounding South Koreans who return from the North, the BBC's Lucy Williamson reports from Seoul. According to South Korea, about 500 of its citizens - most of them fishermen - have been abducted by North Korea since the Korean War. Those kidnapped were often used for propaganda activities or intelligence gathering, Yonhap said. One of the most well-known abduction cases involved a film director and his actress wife, who were abducted by North Korea in the late 1970s. Late film director Shin Sang-ok and his wife, Choe Eun-hui, managed to escape in 1986 while attending a film festival in Vienna. North Korea is also known to have abducted a number of foreign nationals, including several Japanese civilians in the 1970-80s to train spies. In 2002, former Japanese Prime Minister Junichiro Koizumi managed to secure the release of five Japanese abductees. Pyongyang says other known Japanese abductees are dead, but Japan is not convinced and wants more information. The issue remains a highly sensitive one between the two nations.\n",
      "Summary: A South Korean man who was abducted by North Korea 41 years ago has managed to return home, officials in Seoul say.\n",
      "\n",
      "Summarize this document: France's Dubuisson carded a 67 to tie with overnight leader Van Zyl of South Africa on 16 under par. McIlroy carded a third straight five under-par 67 to move to 15 under par with Thailand's Kiradech Aphibarnrat. The world number three's round included an eagle on the 12th as he bids to win his first title since May. \"The 67s I've shot this week have all been a little different and I feel like I've played within myself for all of them, \" said four-time major winner McIlroy of Northern Ireland. \"I feel there's a low round out there for me and hopefully it's tomorrow.\" McIlroy was level par for the day after 10 holes, dropping his first shots of the week by three-putting the third and 10th, the latter mistake prompting the 26-year-old to throw his putter at his bag. But he hit back with a birdie on the par-five 11th and a towering four iron from 229 yards on the 13th set up an eagle from just four feet. The former world number one ruptured a ligament in his left ankle during a game of football with friends in July, ruling him out of several tournaments. But he returned in time to unsuccessfully defend his US PGA title at Whistling Straits in August and played in three of the FedEx Cup play-off events before starting the new PGA Tour season with a tie for 26th in the Frys.com Open in California. He is targeting a third Race to Dubai title in four years and leads England's Danny Willett by 271, 214 points with three events remaining after the Turkish Open. English pair Chris Wood (-13) and Richard Bland (-12) who were tied for second overnight are fifth and seventh respectively.\n",
      "Summary: Rory McIlroy moved to within a shot of joint leaders Victor Dubuisson and Jaco van Zyl after the third round of the Turkish Airlines Open.\n"
     ]
    }
   ],
   "source": [
    "print(xsum_true_false[0]['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.DataFrame({\n",
    "    \"statement\": [rec['statement'] for rec in xsum_true_false],\n",
    "    \"label\": [rec['label'] for rec in xsum_true_false],\n",
    "})\n",
    "csv.to_csv(\"xsum_true_false_TT.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'passage'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer', 'passage'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "boolq = load_dataset(\"/Dataset/google/boolq\")\n",
    "boolq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'do iran and afghanistan speak the same language',\n",
       " 'answer': True,\n",
       " 'passage': 'Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "BOOLQ_TEMPLATE = \"\"\"Passage: {passage}\n",
    "Question: {question}?\n",
    "Options:\n",
    "- Yes\n",
    "- No\n",
    "Answer: {answer}\"\"\"\n",
    "\n",
    "\n",
    "boolq_true_false = []\n",
    "n_shots = 1\n",
    "few_shot_rng = np.random.RandomState(0)\n",
    "\n",
    "def get_few_shot_prompt(few_shot_dataset, rng, n_shots):\n",
    "    few_shot_indices = rng.choice(len(few_shot_dataset), n_shots, replace=False).tolist()\n",
    "    few_shot_prompt = \"\"\n",
    "    for i, ind in enumerate(few_shot_indices):\n",
    "        rec = few_shot_dataset[ind]\n",
    "        if i <= -100:\n",
    "            text = BOOLQ_TEMPLATE.format(passage=rec['passage'], question=rec['question'], answer=\"Yes\" if rec['answer'] else \"No\")\n",
    "        else:\n",
    "            text = BOOLQ_TEMPLATE.format(passage=rec['passage'], question=rec['question'], answer=\"No\" if rec['answer'] else \"Yes\")\n",
    "        few_shot_prompt += text + \"\\n\\n\"\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "for rec in boolq['validation']:\n",
    "    # prompt = BOOLQ_TEMPLATE.format(passage=rec['passage'], question=rec['question'], answer=\"Yes\" if rec['answer'] else \"No\")\n",
    "    few_shot_prompt = get_few_shot_prompt(boolq['train'], few_shot_rng, n_shots)\n",
    "    prompt = few_shot_prompt + BOOLQ_TEMPLATE.format(passage=rec['passage'], question=rec['question'], answer=\"Yes\" if rec['answer'] else \"No\")\n",
    "    \n",
    "    boolq_true_false.append({\n",
    "        \"statement\": prompt,\n",
    "        \"label\": 1,\n",
    "    })\n",
    "    \n",
    "    # prompt = BOOLQ_TEMPLATE.format(passage=rec['passage'], question=rec['question'], answer=\"No\" if rec['answer'] else \"Yes\")\n",
    "    prompt = few_shot_prompt + BOOLQ_TEMPLATE.format(passage=rec['passage'], question=rec['question'], answer=\"No\" if rec['answer'] else \"Yes\")\n",
    "    boolq_true_false.append({\n",
    "        \"statement\": prompt,\n",
    "        \"label\": 0,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: Firearms regulations are uniform throughout Florida, and a carry license is valid everywhere other than in a few specially-defined areas. These prohibited places include any police station, prison, courthouse, polling place, government meeting place, airport, seaport, or tavern. Concealed carry is also prohibited in any school, except for authorized security personnel or armed marshals.\n",
      "Question: can you carry a concealed weapon in florida?\n",
      "Options:\n",
      "- Yes\n",
      "- No\n",
      "Answer: No\n",
      "\n",
      "Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "Question: does ethanol take more energy make that produces?\n",
      "Options:\n",
      "- Yes\n",
      "- No\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "print(boolq_true_false[0]['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.DataFrame({\n",
    "    \"statement\": [rec['statement'] for rec in boolq_true_false],\n",
    "    \"label\": [rec['label'] for rec in boolq_true_false],\n",
    "})\n",
    "csv.to_csv(\"boolq_true_false_with_options_F.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reptda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
